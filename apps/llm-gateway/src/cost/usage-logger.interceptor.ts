import {
  Injectable,
  NestInterceptor,
  ExecutionContext,
  CallHandler,
  Logger,
} from '@nestjs/common';
import { Observable } from 'rxjs';
import { tap } from 'rxjs/operators';
import { Request } from 'express';
import { ChatInDto } from '../chat/dto/chat-in.dto';
import { TimeUtil } from '@libs/common/utils/time.util';

/**
 * LLM ì‚¬ìš©ëŸ‰ ë¡œê¹… Interceptor
 *
 * ëª¨ë“  LLM ìš”ì²­/ì‘ë‹µì„ ê°€ë¡œì±„ì„œ:
 * - ë¹„ìš© (USD)
 * - í† í° ì‚¬ìš©ëŸ‰
 * - ëª¨ë¸ ì •ë³´
 * - ì‘ë‹µ ì‹œê°„
 * - ìºì‹œ íˆíŠ¸ ì—¬ë¶€
 * ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.
 *
 * í–¥í›„ í™•ì¥:
 * - DBì— ì €ì¥í•˜ì—¬ ëŒ€ì‹œë³´ë“œ ìƒì„±
 * - ì•Œë¦¼ (ë¹„ìš© ì„ê³„ê°’ ì´ˆê³¼ ì‹œ)
 * - ì‚¬ìš©ìë³„/ëª¨ë¸ë³„ í†µê³„
 */
@Injectable()
export class UsageLoggerInterceptor implements NestInterceptor {
  private readonly logger = new Logger('LLMUsageTracker');

  intercept(context: ExecutionContext, next: CallHandler): Observable<any> {
    const request = context.switchToHttp().getRequest<Request>();
    const { method, url, body, ip } = request;

    const startTime = Date.now();

    // ìš”ì²­ ë¡œê¹…
    this.logger.log(
      `â†’ LLM Request: ${method} ${url} | ` +
        `IP: ${ip} | ` +
        `Provider: ${body?.provider || 'default'}`,
    );

    return next.handle().pipe(
      tap({
        next: (chatInDto: ChatInDto) => {
          const duration = TimeUtil.diff(startTime);

          // LLM ì‘ë‹µì¸ì§€ í™•ì¸ (cost í•„ë“œë¡œ íŒë³„)
          if (chatInDto && typeof chatInDto.cost === 'number') {
            this.logger.log(
              `âœ“ LLM Response: ${method} ${url} | ` +
                `Provider: ${body?.provider || 'openai'} | ` +
                `Model: ${chatInDto.model} | ` +
                `Tokens: ${chatInDto.tokensUsed.total} ` +
                `(prompt: ${chatInDto.tokensUsed.prompt}, completion: ${chatInDto.tokensUsed.completion}) | ` +
                `Cost: $${chatInDto.cost.toFixed(6)} | ` +
                `Duration: ${TimeUtil.format(duration)} | ` +
                `Cached: ${chatInDto.cached ? 'ğŸ¯ HIT' : 'âŒ MISS'}`,
            );

            // TODO: í–¥í›„ DB ì €ì¥
            // await this.saveUsageToDatabase({
            //   timestamp: new Date(),
            //   provider: body?.provider || 'openai',
            //   model: data.model,
            //   tokensUsed: data.tokensUsed,
            //   cost: data.cost,
            //   duration,
            //   cached: data.cached,
            //   endpoint: url,
            // });

            // TODO: ë¹„ìš© ì•Œë¦¼ (ì„ê³„ê°’ ì´ˆê³¼ ì‹œ)
            // if (data.cost > 0.1) {
            //   this.logger.warn(`âš ï¸ High cost request: $${data.cost}`);
            // }
          } else {
            // LLM ì‘ë‹µì´ ì•„ë‹Œ ê²½ìš° (health check ë“±)
            this.logger.log(
              `âœ“ Response: ${method} ${url} | Duration: ${TimeUtil.format(
                duration,
              )}`,
            );
          }
        },
        error: (error: Error) => {
          const duration = TimeUtil.diff(startTime);

          this.logger.error(
            `âœ— LLM Error: ${method} ${url} | ` +
              `Duration: ${TimeUtil.format(duration)} | ` +
              `Error: ${error.message}`,
            error.stack,
          );

          // TODO: ì—ëŸ¬ ì•Œë¦¼
          // await this.sendErrorAlert({
          //   endpoint: url,
          //   error: error.message,
          //   timestamp: new Date(),
          // });
        },
      }),
    );
  }

  /**
   * (í–¥í›„ í™•ì¥) DBì— ì‚¬ìš©ëŸ‰ ì €ì¥
   */
  // private async saveUsageToDatabase(usage: LlmUsageLog): Promise<void> {
  //   try {
  //     await this.usageRepository.save(usage);
  //   } catch (error) {
  //     this.logger.error(`Failed to save usage log: ${error.message}`);
  //   }
  // }
}

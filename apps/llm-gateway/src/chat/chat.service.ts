import { Injectable, Logger, BadRequestException } from '@nestjs/common';
import { ProviderFactory } from '../providers/provider.factory';
import {
  PromptTemplateService,
  SwingAnalysisData,
} from '../prompt/prompt-template.service';
// import { LLMCacheService } from '../cache/llm-cache.service';
import { ChatOutDto } from './dto/chat-out.dto';
import { ChatInDto } from './dto/chat-in.dto';
import { LLMProviderPort } from '@libs/common/ports/outbound/llm-provider.port';

/**
 * ì±„íŒ… ìš”ì²­ì˜ í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤
 * - í”„ë¡œë°”ì´ë” ì„ íƒ (ë™ì )
 * - ìºì‹œ í™•ì¸
 * - í”„ë¡¬í”„íŠ¸ ìƒì„±
 * - LLM í˜¸ì¶œ
 * - ìºì‹œ ì €ì¥
 */
@Injectable()
export class ChatService {
  private readonly logger = new Logger(ChatService.name);

  constructor(
    private readonly providerFactory: ProviderFactory, // ë™ì  íŒ©í† ë¦¬ ì£¼ì…
    private readonly promptTemplate: PromptTemplateService, // í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸° ì£¼ì… // private readonly cacheService: LLMCacheService, // ìºì‹œ ì„œë¹„ìŠ¤ ì£¼ì…
  ) {}

  /**
   * ì»¨íŠ¸ë¡¤ëŸ¬ë¡œë¶€í„° DTOë¥¼ ë°›ì•„ LLM ì±„íŒ… ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
   */
  async processChat(chatOutDto: ChatOutDto): Promise<ChatInDto> {
    // 1. DTOì—ì„œ í”„ë¡œë°”ì´ë” ì´ë¦„ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ 'openai'ê°€ ê¸°ë³¸ê°’)
    const providerName = chatOutDto.provider || 'openai';

    this.logger.log(
      `LLM ìš”ì²­ ìˆ˜ì‹ : Provider=${providerName}, Language=${
        chatOutDto.language || 'ko'
      }`,
    );

    // // 2. ìºì‹œ í‚¤ ìƒì„±
    // const cacheKey = this.cacheService.generateKey(chatOutDto);
    //
    // // 3. ìºì‹œ í™•ì¸
    // try {
    //   const cached = await this.cacheService.get(cacheKey);
    //   if (cached) {
    //     this.logger.log(`ìºì‹œ íˆíŠ¸ ğŸ¯: ${cacheKey}`);
    //     return { ...cached, cached: true }; // ìºì‹œëœ ì‘ë‹µ ë°˜í™˜
    //   }
    //   this.logger.log(`ìºì‹œ ë¯¸ìŠ¤ miss: ${cacheKey}`);
    // } catch (cacheError) {
    //   this.logger.error(
    //     `ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: ${cacheError.message}`,
    //     cacheError.stack,
    //   );
    //   // ìºì‹œ ì„œë²„ì— ì¥ì• ê°€ ë‚˜ë„, LLMì„ í˜¸ì¶œí•˜ì—¬ ì„œë¹„ìŠ¤ëŠ” ê³„ì†ë˜ì–´ì•¼ í•¨.
    // }

    // --- (ìºì‹œ ë¯¸ìŠ¤ ì‹œ, LLM í˜¸ì¶œ ë¡œì§) ---

    // 4. ë™ì ìœ¼ë¡œ í”„ë¡œë°”ì´ë”(ì–´ëŒ‘í„°) ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    let llmProvider: LLMProviderPort;
    try {
      llmProvider = this.providerFactory.getProvider(providerName);
    } catch (factoryError) {
      this.logger.warn(`í”„ë¡œë°”ì´ë” ì„ íƒ ì‹¤íŒ¨: ${factoryError.message}`);
      throw new BadRequestException(factoryError.message); // 400 ì—ëŸ¬
    }

    // 5. í”„ë¡¬í”„íŠ¸ ìƒì„±
    // (DTOì˜ analysisDataê°€ SwingAnalysisData íƒ€ì…ê³¼ í˜¸í™˜ë˜ì–´ì•¼ í•¨)
    const prompt = this.promptTemplate.buildSwingAnalysisPrompt(
      chatOutDto.analysisData as SwingAnalysisData, // íƒ€ì… ë‹¨ì–¸
      chatOutDto.language || 'ko',
    );

    // 6. LLM í˜¸ì¶œ
    // (Adapterì—ì„œ ì—ëŸ¬ ë°œìƒ ì‹œ, Adapterê°€ throwí•œ ì˜ˆì™¸ê°€
    //  NestJSì˜ Exception Filterì— ì˜í•´ ì²˜ë¦¬ë¨)
    const llmResponse = await llmProvider.chat({
      messages: [
        // OpenAI/Claude ëª¨ë‘ System í”„ë¡¬í”„íŠ¸ë¥¼ messages ë°–ì´ë‚˜ ì•ˆìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥
        // ì—¬ê¸°ì„œëŠ” PromptTemplateServiceê°€ ì´ë¯¸ System + Userë¥¼ í•©ì³¤ë‹¤ê³  ê°€ì •
        { role: 'system', content: prompt },
        // User í”„ë¡¬í”„íŠ¸ë¥¼ ë¶„ë¦¬í•˜ê³  ì‹¶ë‹¤ë©´ PromptTemplateService ìˆ˜ì • í•„ìš”
        {
          role: 'user',
          content: 'ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.',
        },
      ],
      model: chatOutDto.model, // DTOì— ëª¨ë¸ì´ ëª…ì‹œë˜ë©´ í•´ë‹¹ ëª¨ë¸ ì‚¬ìš©
      temperature: chatOutDto.temperature ?? 0.7,
      maxTokens: 2000,
      language: chatOutDto.language,
    });

    // 7. ìµœì¢… ì‘ë‹µ DTO êµ¬ì„±
    const chatInDto: ChatInDto = {
      feedback: llmResponse.content,
      model: llmResponse.model,
      tokensUsed: llmResponse.tokensUsed,
      cost: llmResponse.cost,
      cached: false, // ìºì‹œ ë¯¸ìŠ¤
    };

    // 8. ìºì‹œì— ì €ì¥ (Non-Blocking)
    // awaitë¥¼ ì“°ì§€ ì•Šê±°ë‚˜, ì¨ë„ ì‘ë‹µ ë°˜í™˜ í›„ ì²˜ë¦¬ë˜ë„ë¡ í•¨.
    // this.cacheService
    //   .set(cacheKey, chatInDto, 3600) // 1ì‹œê°„ ìºì‹œ
    //   .catch((err) => {
    //     this.logger.error(`ìºì‹œ ì €ì¥ ì‹¤íŒ¨: ${err.message}`, cacheKey);
    //   });

    this.logger.log(
      `LLM ì‘ë‹µ ì™„ë£Œ (${providerName}): í† í°=${
        llmResponse.tokensUsed.total
      }, ë¹„ìš©=$${llmResponse.cost?.toFixed(6)}`,
    );

    return chatInDto;
  }
}

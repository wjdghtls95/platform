# ğŸ¤– LLM Gateway Service Documentation

**apps/llm-gateway** - LLM API ì¤‘ì•™ ê´€ë¦¬ ë° í”„ë¡ì‹œ ì„œë¹„ìŠ¤

---

## ğŸ“‘ ëª©ì°¨

1. [ì„œë¹„ìŠ¤ ê°œìš”](#-ì„œë¹„ìŠ¤-ê°œìš”)
2. [ê¸°ìˆ  ìŠ¤íƒ](#-ê¸°ìˆ -ìŠ¤íƒ)
3. [ë””ë ‰í† ë¦¬ êµ¬ì¡°](#-ë””ë ‰í† ë¦¬-êµ¬ì¡°)
4. [ì£¼ìš” ëª¨ë“ˆ](#-ì£¼ìš”-ëª¨ë“ˆ)
5. [API ì—”ë“œí¬ì¸íŠ¸](#-api-ì—”ë“œí¬ì¸íŠ¸)
6. [í™˜ê²½ ë³€ìˆ˜](#-í™˜ê²½-ë³€ìˆ˜)
7. [ë¡œì»¬ ê°œë°œ](#-ë¡œì»¬-ê°œë°œ)
8. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## ğŸ¯ ì„œë¹„ìŠ¤ ê°œìš”

### ì±…ì„ ë²”ìœ„

LLM Gateway ì„œë¹„ìŠ¤ëŠ” **ì—¬ëŸ¬ LLM APIë¥¼ í†µí•© ê´€ë¦¬**í•˜ëŠ” ì¤‘ì•™ í”„ë¡ì‹œ ì„œë²„ì…ë‹ˆë‹¤.

**ë‹´ë‹¹ ë„ë©”ì¸:**

- ğŸ¤– **LLM API í†µí•©**: OpenAI, Claude ë“± ì—¬ëŸ¬ ì œê³µì í†µí•©
- ğŸ’° **ë¹„ìš© ìµœì í™”**: Redis ìºì‹±ìœ¼ë¡œ ë™ì¼ ìš”ì²­ ë¹„ìš© ì ˆê°
- ğŸ“Š **ì‚¬ìš©ëŸ‰ ì¶”ì **: í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ë¡œê¹…
- ğŸ¯ **í”„ë¡¬í”„íŠ¸ ê´€ë¦¬**: í…œí”Œë¦¿ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±

**ì™¸ë¶€ ì˜ì¡´ì„±:**

- OpenAI API
- Anthropic Claude API (ì„ íƒ)
- Redis (ìºì‹±, í˜„ì¬ ì£¼ì„ ì²˜ë¦¬)

**ì„¤ê³„ ì² í•™:**

- **ë‹¨ì¼ ì§„ì…ì **: ëª¨ë“  LLM ìš”ì²­ì´ ì´ ì„œë¹„ìŠ¤ë¥¼ ê²½ìœ 
- **ì œê³µì ì¶”ìƒí™”**: Adapter íŒ¨í„´ìœ¼ë¡œ LLM ì œê³µì ì‰½ê²Œ ì¶”ê°€
- **ë¹„ìš© ê´€ë¦¬**: ì‚¬ìš©ëŸ‰ ë¡œê¹…ìœ¼ë¡œ ë¹„ìš© ì¶”ì 

---

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

| ì¹´í…Œê³ ë¦¬         | ê¸°ìˆ                                                                                                                                                                                                                          |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Framework**    | ![NestJS](https://img.shields.io/badge/NestJS_10.x-E0234E?style=flat-square&logo=nestjs&logoColor=white)                                                                                                                     |
| **Language**     | ![TypeScript](https://img.shields.io/badge/TypeScript_5.x-3178C6?style=flat-square&logo=typescript&logoColor=white) ![Node.js](https://img.shields.io/badge/Node.js-339933?style=flat-square&logo=nodedotjs&logoColor=white) |
| **LLM APIs**     | ![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=flat-square&logo=openai&logoColor=white) ![Claude](https://img.shields.io/badge/Claude-000000?style=flat-square)                                                  |
| **Cache**        | ![Redis](https://img.shields.io/badge/Redis_7.x-DC382D?style=flat-square&logo=redis&logoColor=white) (í˜„ì¬ ì£¼ì„ ì²˜ë¦¬)                                                                                                        |
| **Architecture** | Factory Pattern + Adapter Pattern                                                                                                                                                                                            |
| **Validation**   | ![Class Validator](https://img.shields.io/badge/class--validator-gray?style=flat-square)                                                                                                                                     |

---

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```plaintext
apps/llm-gateway/src/
â”œâ”€â”€ chat/                           # ì±„íŒ… API ëª¨ë“ˆ
â”‚   â”œâ”€â”€ chat.controller.ts          # API ì—”ë“œí¬ì¸íŠ¸
â”‚   â”œâ”€â”€ chat.service.ts             # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
â”‚   â”œâ”€â”€ dto/                        # ìš”ì²­/ì‘ë‹µ DTO
â”‚   â”‚   â”œâ”€â”€ chat-in.dto.ts          # ìš”ì²­(Input) DTO
â”‚   â”‚   â””â”€â”€ chat-out.dto.ts         # ì‘ë‹µ(Output) DTO
â”‚
â”œâ”€â”€ providers/                      # LLM ì œê³µì ì–´ëŒ‘í„°
â”‚   â”œâ”€â”€ provider.factory.ts         # Factory Pattern
â”‚   â”œâ”€â”€ openai.adapter.ts           # OpenAI êµ¬í˜„ì²´
â”‚   â””â”€â”€ claude.adapter.ts           # Claude êµ¬í˜„ì²´
â”‚
â”œâ”€â”€ prompt/                         # í”„ë¡¬í”„íŠ¸ ê´€ë¦¬
â”‚   â””â”€â”€ prompt-template.service.ts  # í…œí”Œë¦¿ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
â”‚
â”œâ”€â”€ cost/                           # ë¹„ìš© ì¶”ì 
â”‚   â””â”€â”€ usage-logger.interceptor.ts # ì‚¬ìš©ëŸ‰ ë¡œê¹… ì¸í„°ì…‰í„°
â”‚
â”œâ”€â”€ cache/                          # ìºì‹± (í˜„ì¬ ì£¼ì„ ì²˜ë¦¬)
â”‚   â””â”€â”€ llm-cache.service.ts        # Redis ìºì‹±
â”‚
â”œâ”€â”€ llm-gateway.module.ts           # ë£¨íŠ¸ ëª¨ë“ˆ
â”œâ”€â”€ llm-gateway.server.ts           # ì„œë²„ ì„¤ì • (Swagger, CORS)
â””â”€â”€ main.ts                         # ì• í”Œë¦¬ì¼€ì´ì…˜ ì§„ì…ì 
```

---

## ğŸ”§ ì£¼ìš” ëª¨ë“ˆ

### 1. Chat ëª¨ë“ˆ (chat/)

#### ê°œìš”

LLM API í˜¸ì¶œì„ ì²˜ë¦¬í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆì…ë‹ˆë‹¤.

#### ì£¼ìš” ê¸°ëŠ¥

- âœ… ì—¬ëŸ¬ LLM ì œê³µì ì§€ì› (OpenAI, Claude)
- âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê¸°ë°˜ ìš”ì²­ ìƒì„±
- âœ… ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ë¡œê¹…
- âœ… Redis ìºì‹± (í˜„ì¬ ë¹„í™œì„±í™”)

---

#### LLM í˜¸ì¶œ í”Œë¡œìš°

```mermaid
sequenceDiagram
    actor Analyzer as Swing Analyzer<br/>(FastAPI)
    participant Gateway as LLM Gateway
    participant Factory as Provider Factory
    participant OpenAI as OpenAI Adapter
    participant API as OpenAI API
    participant Logger as Usage Logger

    Analyzer->>+Gateway: 1. POST /chat<br/>Header: X-Internal-API-Key

    Gateway->>Gateway: 2. API í‚¤ ê²€ì¦
    Gateway->>Gateway: 3. DTO ìœ íš¨ì„± ê²€ì‚¬

    Gateway->>Factory: 4. getProvider('openai')
    Factory-->>Gateway: 5. OpenAI Adapter ë°˜í™˜

    Gateway->>Gateway: 6. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±<br/>(analysisData + language)

    Gateway->>+OpenAI: 7. chat(messages, model, temp)
    OpenAI->>+API: 8. API í˜¸ì¶œ (gpt-4o-mini)
    API-->>-OpenAI: 9. { content, usage, cost }
    OpenAI-->>-Gateway: 10. LLM ì‘ë‹µ

    Gateway->>Logger: 11. ì‚¬ìš©ëŸ‰ ë¡œê¹…<br/>(í† í°, ë¹„ìš©, ì‹œê°„)

    Gateway-->>-Analyzer: 12. { feedback, model, tokensUsed, cost }
```

---

### í•µì‹¬ ì½”ë“œ

**chat.controller.ts**

```ts
import {
  Controller,
  Post,
  Body,
  UseInterceptors,
  HttpCode,
  HttpStatus,
} from '@nestjs/common';
import { ApiTags, ApiOperation, ApiHeader } from '@nestjs/swagger';
import { ChatService } from './chat.service';
import { ChatOutDto } from './dto/chat-out.dto';
import { ChatInDto } from './dto/chat-in.dto';
import { UsageLoggerInterceptor } from '../cost/usage-logger.interceptor';
import { ApiKeyAuth } from '@libs/common/decorators/api-key-auth.decorator';

@ApiTags('LLM Gateway')
@Controller()
@ApiKeyAuth()
@UseInterceptors(UsageLoggerInterceptor) // ì‚¬ìš©ëŸ‰ ë¡œê¹…
export class ChatController {
  constructor(private readonly chatService: ChatService) {}

  /**
   * LLM ì±„íŒ… ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸
   */
  @Post('chat')
  @HttpCode(HttpStatus.OK)
  @ApiOperation({ summary: 'LLM ì±„íŒ… ìš”ì²­ (ë‚´ë¶€ ì „ìš©)' })
  @ApiHeader({
    name: 'X-Internal-API-Key',
    description: 'ë‚´ë¶€ ì„œë¹„ìŠ¤ ì¸ì¦ í‚¤',
    required: true,
  })
  async chat(@Body() chatOutDto: ChatOutDto): Promise<ChatInDto> {
    return await this.chatService.processChat(chatOutDto);
  }
}
```

**chat.service.ts (í•µì‹¬ ë¡œì§)**

```ts
import { Injectable, Logger, BadRequestException } from '@nestjs/common';
import { ProviderFactory } from '../providers/provider.factory';
import {
  PromptTemplateService,
  SwingAnalysisData,
} from '../prompt/prompt-template.service';
import { ChatOutDto } from './dto/chat-out.dto';
import { ChatInDto } from './dto/chat-in.dto';
import { LLMProviderPort } from '@libs/common/ports/outbound/llm-provider.port';

@Injectable()
export class ChatService {
  private readonly logger = new Logger(ChatService.name);

  constructor(
    private readonly providerFactory: ProviderFactory,
    private readonly promptTemplate: PromptTemplateService,
  ) {}

  async processChat(chatOutDto: ChatOutDto): Promise<ChatInDto> {
    const providerName = chatOutDto.provider || 'openai';

    this.logger.log(
      `LLM ìš”ì²­ ìˆ˜ì‹ : Provider=${providerName}, Language=${
        chatOutDto.language || 'ko'
      }`,
    );

    // 1. í”„ë¡œë°”ì´ë”(ì–´ëŒ‘í„°) ê°€ì ¸ì˜¤ê¸°
    let llmProvider: LLMProviderPort;
    try {
      llmProvider = this.providerFactory.getProvider(providerName);
    } catch (factoryError) {
      this.logger.warn(`í”„ë¡œë°”ì´ë” ì„ íƒ ì‹¤íŒ¨: ${factoryError.message}`);
      throw new BadRequestException(factoryError.message);
    }

    // 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
    const prompt = this.promptTemplate.buildSwingAnalysisPrompt(
      chatOutDto.analysisData as SwingAnalysisData,
      chatOutDto.language || 'ko',
    );

    // 3. LLM í˜¸ì¶œ
    const llmResponse = await llmProvider.chat({
      messages: [
        { role: 'system', content: prompt },
        {
          role: 'user',
          content: 'ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.',
        },
      ],
      model: chatOutDto.model,
      temperature: chatOutDto.temperature ?? 0.7,
      maxTokens: 2000,
      language: chatOutDto.language,
    });

    // 4. ìµœì¢… ì‘ë‹µ DTO êµ¬ì„±
    const chatInDto: ChatInDto = {
      feedback: llmResponse.content,
      model: llmResponse.model,
      tokensUsed: llmResponse.tokensUsed,
      cost: llmResponse.cost,
      cached: false,
    };

    this.logger.log(
      `LLM ì‘ë‹µ ì™„ë£Œ (${providerName}): í† í°=${
        llmResponse.tokensUsed.total
      }, ë¹„ìš©=$${llmResponse.cost?.toFixed(6)}`,
    );

    return chatInDto;
  }
}
```

---

### 2. Provider Factory & Adapters (providers/)

#### ê°œìš”

Factory íŒ¨í„´ìœ¼ë¡œ ì—¬ëŸ¬ LLM ì œê³µìë¥¼ ë™ì ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.

#### ì§€ì› ì œê³µì

- âœ… **OpenAI** (ê¸°ë³¸ê°’: gpt-4o-mini)
- âœ… **Claude** (anthropic/claude-3-5-sonnet-20241022)

**provider.factory.ts**

```ts
import { Injectable } from '@nestjs/common';
import { LLMProviderPort } from '@libs/common/ports/outbound/llm-provider.port';
import { OpenAIAdapter } from './openai.adapter';
import { ClaudeAdapter } from './claude.adapter';

@Injectable()
export class ProviderFactory {
  constructor(
    private readonly openaiAdapter: OpenAIAdapter,
    private readonly claudeAdapter: ClaudeAdapter,
  ) {}

  /**
   * í”„ë¡œë°”ì´ë” ì´ë¦„ìœ¼ë¡œ ì–´ëŒ‘í„° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
   */
  getProvider(providerName: string): LLMProviderPort {
    switch (providerName.toLowerCase()) {
      case 'openai':
        return this.openaiAdapter;
      case 'claude':
        return this.claudeAdapter;
      default:
        throw new Error(`ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: ${providerName}`);
    }
  }
}
```

**openai.adapter.ts (í•µì‹¬ ë¶€ë¶„)**

```ts
import {
  Injectable,
  InternalServerErrorException,
  Logger,
} from '@nestjs/common';
import {
  LLMProviderPort,
  LLMRequest,
  LLMResponse,
} from '@libs/common/ports/outbound/llm-provider.port';
import OpenAI from 'openai';
import { ConfigService } from '@nestjs/config';

@Injectable()
export class OpenAIAdapter implements LLMProviderPort {
  readonly providerName = 'openai';
  private readonly logger = new Logger(OpenAIAdapter.name);
  private client: OpenAI;

  constructor(private configService: ConfigService) {
    const apiKey = this.configService.get<string>('LLM_OPENAI_API_KEY');

    if (!apiKey) {
      throw new Error('OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
    }

    this.client = new OpenAI({ apiKey });
  }

  async chat(request: LLMRequest): Promise<LLMResponse> {
    const startTime = Date.now();
    const model = request.model || 'gpt-4o-mini';

    try {
      const completion = await this.client.chat.completions.create({
        model: model,
        messages: request.messages,
        temperature: request.temperature ?? 0.7,
        max_tokens: request.maxTokens,
      });

      const response = completion.choices[0].message;
      const usage = completion.usage;

      if (!response || !usage) {
        throw new InternalServerErrorException('OpenAI ì‘ë‹µì´ ë¹„ì •ìƒì…ë‹ˆë‹¤.');
      }

      // ë¹„ìš© ê³„ì‚° (gpt-4o-mini ê¸°ì¤€: $0.15/1M input, $0.60/1M output)
      const cost =
        (usage.prompt_tokens * 0.15) / 1_000_000 +
        (usage.completion_tokens * 0.6) / 1_000_000;

      this.logger.log(
        `OpenAI (${model}) ìš”ì²­ ì™„ë£Œ: ${Date.now() - startTime}ms, ` +
          `í† í°: ${usage.total_tokens}, ë¹„ìš©: $${cost.toFixed(6)}`,
      );

      return {
        content: response.content || '',
        model: completion.model,
        tokensUsed: {
          prompt: usage.prompt_tokens,
          completion: usage.completion_tokens,
          total: usage.total_tokens,
        },
        cost,
      };
    } catch (error) {
      this.logger.error(`OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: ${error.message}`, error.stack);
      throw new InternalServerErrorException(
        `OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: ${error.message}`,
      );
    }
  }
}
```

---

### 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (prompt/)

#### ê°œìš”

ìŠ¤ìœ™ ë¶„ì„ ë°ì´í„°ë¥¼ LLM í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

**prompt-template.service.ts (ì˜ˆì‹œ)**

```ts
import { Injectable } from '@nestjs/common';

export interface SwingAnalysisData {
  backswingAngle: number;
  downswingAngle: number;
  impact: {
    clubFaceAngle: number;
    clubPath: string;
  };
  // ... ê¸°íƒ€ ë¶„ì„ ë°ì´í„°
}

@Injectable()
export class PromptTemplateService {
  /**
   * ìŠ¤ìœ™ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
   */
  buildSwingAnalysisPrompt(
    data: SwingAnalysisData,
    language: string = 'ko',
  ): string {
    const langInstruction =
      language === 'ko'
        ? 'í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.'
        : 'Please respond in English.';

    return `
ë‹¹ì‹ ì€ ê³¨í”„ ìŠ¤ìœ™ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ${langInstruction}

ë‹¤ìŒ ë¶„ì„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”:

- ë°±ìŠ¤ìœ™ ê°ë„: ${data.backswingAngle}ë„
- ë‹¤ìš´ìŠ¤ìœ™ ê°ë„: ${data.downswingAngle}ë„
- ì„íŒ©íŠ¸ ì‹œ í´ëŸ½ í˜ì´ìŠ¤ ê°ë„: ${data.impact.clubFaceAngle}ë„
- í´ëŸ½ ê²½ë¡œ: ${data.impact.clubPath}

**í”¼ë“œë°± í˜•ì‹:**
1. ì£¼ìš” ë¬¸ì œì  (1-2ê°€ì§€)
2. ê°œì„  ë°©ë²• (êµ¬ì²´ì ì¸ ë™ì‘)
3. ì¶”ì²œ ì—°ìŠµ ë°©ë²•
`.trim();
  }
}
```

---

### 4. ì‚¬ìš©ëŸ‰ ë¡œê¹… (cost/)

#### ê°œìš”

ëª¨ë“  LLM ìš”ì²­ì˜ í† í° ì‚¬ìš©ëŸ‰ê³¼ ë¹„ìš©ì„ ë¡œê¹…í•©ë‹ˆë‹¤.

**usage-logger.interceptor.ts**

```ts
import {
  Injectable,
  NestInterceptor,
  ExecutionContext,
  CallHandler,
  Logger,
} from '@nestjs/common';
import { Observable } from 'rxjs';
import { tap } from 'rxjs/operators';

@Injectable()
export class UsageLoggerInterceptor implements NestInterceptor {
  private readonly logger = new Logger(UsageLoggerInterceptor.name);

  intercept(context: ExecutionContext, next: CallHandler): Observable<any> {
    const request = context.switchToHttp().getRequest();
    const startTime = Date.now();

    return next.handle().pipe(
      tap((response) => {
        const duration = Date.now() - startTime;

        // ì‘ë‹µì—ì„œ ë¹„ìš© ì •ë³´ ì¶”ì¶œ
        const { model, tokensUsed, cost } = response;

        this.logger.log(
          `ğŸ“Š LLM ì‚¬ìš©ëŸ‰: ëª¨ë¸=${model}, ` +
            `í† í°=${tokensUsed?.total || 0}, ` +
            `ë¹„ìš©=$${cost?.toFixed(6) || 0}, ` +
            `ì‘ë‹µì‹œê°„=${duration}ms`,
        );
      }),
    );
  }
}
```

---

## ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸

### Swagger API ë¬¸ì„œ

| ë©”ì„œë“œ | ê²½ë¡œ        | ì„¤ëª…                            | ì¸ì¦ |
| ------ | ----------- | ------------------------------- | ---- |
| GET    | `/api-docs` | Swagger UI (API ë¬¸ì„œ ë° í…ŒìŠ¤íŠ¸) | âŒ   |

> **ğŸ’¡ Tip**: Swagger UIì—ì„œ ëª¨ë“  APIë¥¼ ì‹œê°ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
> ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:${LLM_GATEWAY_SERVER_PORT}/api-docs` ì ‘ì† í›„ ìš°ì¸¡ ìƒë‹¨ **"Authorize"** ë²„íŠ¼ìœ¼ë¡œ `X-Internal-API-Key`ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

---

### LLM Gateway ì„œë¹„ìŠ¤ API ëª©ë¡

### ì±„íŒ… (chat/)

| ë©”ì„œë“œ | ê²½ë¡œ    | ì„¤ëª…                      | ì¸ì¦                |
| ------ | ------- | ------------------------- | ------------------- |
| POST   | `/chat` | LLM ì±„íŒ… ìš”ì²­ (ë‚´ë¶€ ì „ìš©) | âœ… Internal API Key |

**ìš”ì²­ ì˜ˆì‹œ:**

```json
{
  "provider": "openai",
  "model": "gpt-4o-mini",
  "temperature": 0.7,
  "language": "ko",
  "analysisData": {
    "backswingAngle": 95,
    "downswingAngle": 85,
    "impact": {
      "clubFaceAngle": 2,
      "clubPath": "inside-out"
    }
  }
}
```

**ì‘ë‹µ ì˜ˆì‹œ:**

```json
{
  "feedback": "ë°±ìŠ¤ìœ™ ì‹œ íŒ”ì´ ë„ˆë¬´ êµ¬ë¶€ëŸ¬ì ¸ ìˆìŠµë‹ˆë‹¤...",
  "model": "gpt-4o-mini-2024-07-18",
  "tokensUsed": {
    "prompt": 150,
    "completion": 200,
    "total": 350
  },
  "cost": 0.000135,
  "cached": false
}
```

---

## âš™ï¸ í™˜ê²½ ë³€ìˆ˜

### í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ìœ„ì¹˜

LLM Gateway ì„œë¹„ìŠ¤ì˜ í™˜ê²½ ë³€ìˆ˜ëŠ” **rootì˜ `config/` ë””ë ‰í† ë¦¬**ì—ì„œ ê´€ë¦¬ë©ë‹ˆë‹¤.

```plaintext
project-root/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ platform.env          # Platform ì„œë¹„ìŠ¤ í™˜ê²½ ë³€ìˆ˜
â”‚   â”œâ”€â”€ integration.env        # Integration ì„œë¹„ìŠ¤ í™˜ê²½ ë³€ìˆ˜
â”‚   â””â”€â”€ llm-gateway.env        # âœ… LLM Gateway ì„œë¹„ìŠ¤ í™˜ê²½ ë³€ìˆ˜ (ì—¬ê¸°!)
```

---

### config/llm-gateway.env ì„¤ì • ì˜ˆì‹œ

```bash
# ===================================
# LLM Gateway ì„œë¹„ìŠ¤ ì„¤ì •
# ===================================

# ì„œë²„ ì„¤ì •
NODE_ENV=development
LLM_GATEWAY_SERVER_PORT=3002    # LLM Gateway ì„œë¹„ìŠ¤ í¬íŠ¸ (ê¸°ë³¸ê°’: 3002)

# ë‚´ë¶€ API ì¸ì¦
INTERNAL_API_KEY=your-internal-api-key  # Swing Analyzerì™€ ë™ì¼í•œ API í‚¤

# OpenAI API
LLM_OPENAI_API_KEY=sk-proj-...  # OpenAI API í‚¤

# Anthropic Claude API (ì„ íƒ)
LLM_CLAUDE_API_KEY=sk-ant-...   # Claude API í‚¤ (ì„ íƒ ì‚¬í•­)

# Redis (ìºì‹±, í˜„ì¬ ë¹„í™œì„±í™”)
# REDIS_HOST=localhost
# REDIS_PORT=6379
```

---

### ì£¼ìš” í™˜ê²½ ë³€ìˆ˜ ì„¤ëª…

| í™˜ê²½ ë³€ìˆ˜                 | ì„¤ëª…                    | ê¸°ë³¸ê°’        | í•„ìˆ˜ |
| ------------------------- | ----------------------- | ------------- | ---- |
| `NODE_ENV`                | ì‹¤í–‰ í™˜ê²½               | `development` | âœ…   |
| `LLM_GATEWAY_SERVER_PORT` | LLM Gateway ì„œë¹„ìŠ¤ í¬íŠ¸ | `3002`        | âœ…   |
| `INTERNAL_API_KEY`        | ì„œë¹„ìŠ¤ ê°„ ì¸ì¦ í‚¤       | -             | âœ…   |
| `LLM_OPENAI_API_KEY`      | OpenAI API í‚¤           | -             | âœ…   |
| `LLM_CLAUDE_API_KEY`      | Claude API í‚¤           | -             | âŒ   |
| `REDIS_HOST`              | Redis ì„œë²„ í˜¸ìŠ¤íŠ¸       | `localhost`   | âŒ   |
| `REDIS_PORT`              | Redis ì„œë²„ í¬íŠ¸         | `6379`        | âŒ   |

---

### í¬íŠ¸ ë²ˆí˜¸ ë³€ê²½ ë°©ë²•

LLM Gateway ì„œë¹„ìŠ¤ì˜ í¬íŠ¸ë¥¼ ë³€ê²½í•˜ë ¤ë©´:

1. **`config/llm-gateway.env` íŒŒì¼ ìˆ˜ì •**

   ```bash
   LLM_GATEWAY_SERVER_PORT=3005  # ì›í•˜ëŠ” í¬íŠ¸ ë²ˆí˜¸ë¡œ ë³€ê²½
   ```

2. **Swing Analyzer(FastAPI)ì—ì„œ LLM Gateway URL ì—…ë°ì´íŠ¸ (í•„ìš” ì‹œ)**

   - Swing Analyzerê°€ LLM Gatewayë¥¼ í˜¸ì¶œí•˜ëŠ” ê²½ìš°, URL ì—…ë°ì´íŠ¸ í•„ìš”

3. **ì„œë¹„ìŠ¤ ì¬ì‹œì‘**

   ```bash
   pnpm run start:dev llm-gateway
   ```

4. **Swagger ì ‘ì†**
   ```
   http://localhost:3005/api-docs  # ë³€ê²½ëœ í¬íŠ¸ë¡œ ì ‘ì†
   ```

---

## ğŸš€ ë¡œì»¬ ê°œë°œ

### 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# 1. config ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd config

# 2. ì˜ˆì œ íŒŒì¼ ë³µì‚¬
cp llm-gateway.env.example llm-gateway.env

# 3. llm-gateway.env íŒŒì¼ ìˆ˜ì •
# - LLM_GATEWAY_SERVER_PORT: LLM Gateway ì„œë¹„ìŠ¤ í¬íŠ¸ (ê¸°ë³¸ê°’: 3002)
# - INTERNAL_API_KEY: Swing Analyzerì™€ ë™ì¼í•œ í‚¤
# - LLM_OPENAI_API_KEY: OpenAI API í‚¤ (í•„ìˆ˜)
# - LLM_CLAUDE_API_KEY: Claude API í‚¤ (ì„ íƒ)
```

---

### 2. LLM Gateway ì‹¤í–‰

```bash
# ê°œë°œ ëª¨ë“œ (Hot Reload)
pnpm run start:dev llm-gateway
# ğŸ‘‰ config/llm-gateway.envì˜ LLM_GATEWAY_SERVER_PORT ì‚¬ìš©

# í”„ë¡œë•ì…˜ ë¹Œë“œ
pnpm run build llm-gateway
pnpm run start:prod llm-gateway
```

**ì ‘ì†:**

- LLM Gateway: `http://localhost:${LLM_GATEWAY_SERVER_PORT}/api-docs` (ê¸°ë³¸ê°’: http://localhost:3002/api-docs)
- Health Check: `http://localhost:${LLM_GATEWAY_SERVER_PORT}/health` (ê¸°ë³¸ê°’: http://localhost:3002/health)

---

### 3. API í…ŒìŠ¤íŠ¸

```bash
# Health Check (config/llm-gateway.envì˜ LLM_GATEWAY_SERVER_PORT ì‚¬ìš©)
curl http://localhost:${LLM_GATEWAY_SERVER_PORT}/health

# ê¸°ë³¸ í¬íŠ¸ ì‚¬ìš© ì‹œ
curl http://localhost:3002/health

# LLM ì±„íŒ… ìš”ì²­
curl -X POST http://localhost:${LLM_GATEWAY_SERVER_PORT}/chat \
  -H "X-Internal-API-Key: your-internal-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "gpt-4o-mini",
    "temperature": 0.7,
    "language": "ko",
    "analysisData": {
      "backswingAngle": 95,
      "downswingAngle": 85,
      "impact": {
        "clubFaceAngle": 2,
        "clubPath": "inside-out"
      }
    }
  }'
```

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- [ğŸ—ï¸ ì „ì²´ ì•„í‚¤í…ì²˜](../README.md)
- [ğŸ“± Platform ì„œë¹„ìŠ¤](./platform.md)
- [ğŸ“§ Integration ì„œë¹„ìŠ¤](./integration.md)
- [ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„](./database.md)
- [ğŸ” ë³´ì•ˆ ì „ëµ](./security.md)

---

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. OpenAI API í‚¤ ì˜¤ë¥˜

```bash
Error: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤

# í•´ê²°:
1. config/llm-gateway.env íŒŒì¼ì— LLM_OPENAI_API_KEY ì¶”ê°€
LLM_OPENAI_API_KEY=sk-proj-...

2. OpenAI ëŒ€ì‹œë³´ë“œì—ì„œ API í‚¤ í™•ì¸
https://platform.openai.com/api-keys
```

---

### 2. ë‚´ë¶€ API í‚¤ ì˜¤ë¥˜

```bash
Error: 401 Unauthorized

# í•´ê²°:
1. ìš”ì²­ í—¤ë”ì— X-Internal-API-Key í¬í•¨ í™•ì¸
2. Swing Analyzer(FastAPI)ì˜ INTERNAL_API_KEYê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
   - config/llm-gateway.envì˜ INTERNAL_API_KEY
   - Swing Analyzer(FastAPI)ì˜ INTERNAL_API_KEY
```

---

### 3. ë¹„ìš© ê³¼ë‹¤ ë°œìƒ

```bash
Warning: OpenAI API ë¹„ìš©ì´ ì˜ˆìƒë³´ë‹¤ ë†’ìŠµë‹ˆë‹¤

# í•´ê²°:
1. Redis ìºì‹± í™œì„±í™” (chat.service.ts ì£¼ì„ ì œê±°)
   - config/llm-gateway.envì— REDIS_HOST, REDIS_PORT ì¶”ê°€

2. ëª¨ë¸ì„ gpt-4o-minië¡œ ë³€ê²½ (ë¹„ìš© 1/10)
   - ìš”ì²­ ì‹œ model íŒŒë¼ë¯¸í„° ë³€ê²½

3. temperature ê°’ì„ ë‚®ì¶°ì„œ í† í° ì‚¬ìš©ëŸ‰ ê°ì†Œ
   - temperature: 0.3 ~ 0.5 ê¶Œì¥
```

---

### 4. Swing Analyzer ì—°ê²° ì˜¤ë¥˜

```bash
Error: connect ECONNREFUSED

# í•´ê²°:
1. LLM Gateway ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
pnpm run start:dev llm-gateway

2. config/llm-gateway.envì˜ LLM_GATEWAY_SERVER_PORT í™•ì¸
LLM_GATEWAY_SERVER_PORT=3002

3. Swing Analyzer(FastAPI)ì˜ LLM Gateway URL í™•ì¸
LLM_GATEWAY_URL=http://localhost:3002  # LLM_GATEWAY_SERVER_PORTì™€ ì¼ì¹˜
```
